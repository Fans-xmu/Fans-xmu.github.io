---
layout:     post
title:      还不知道怎么提示LLM？ChatGPT提示入门
subtitle:   NLP必学
date:       2023-08-18
author:     Fans
header-img: img/post-bg-os-metro.jpg
catalog: 	  true
tags:
    - 深度学习

---
# 简介：
## 什么是人工智能？
从目的的角度出发，人工智能（AI）是人们创造的一些“聪明的”的算法，使得机器能够像人类一样“思考”。这些算法可以写论文、解决数学问题，并创造艺术。该领域的最新进展已经十分先进，以至于人工智能可以写出令人信服的销售邮件、新闻报道，甚至可以赢得艺术比赛。
## 什么是提示过程？
提示过程指的是人类如何指导 AI 执行任务，即指导人工智能执行任务的过程称为提示过程。我们向 AI 提供一组指令（提示），然后它执行任务。提示可以简单到一个问题，也可以复杂到多个段落。
而提示过程的好坏可以在很大程度上影响到AI大模型生成答案的能力！

## 为什么会出现这样的差异？
因为LLM本质上是自回归的语言模型，而自回归的语言模型训练是通过Decoder对每个词的位置只可见前面位置的词这样的Mask Self-Attention的形式来计算并生成的，每个词对后面未来的词是不可见的，简而言之，就是当前词的生成依赖于前面已经生成的文本，而这个过程是自回归模型结构决定的。
举个例子，如下图：

![在这里插入图片描述](https://img-blog.csdnimg.cn/5feccd87cc33416d874b239e7b8e3645.png)

在自回归模型解码生成上面一句话的过程中，解码器是如何训练计算loss的呢？
\<s>,**北，京，欢，迎，你**<e>这个例子做生成过程来解释。(\<s>为起始词，<e>为结束词)
训练时：
把“\<s> **北，京，欢，迎，你**”的word embedding和position embedding处理后后输入到decoder中去，而用于指导其每个位置生成的ground-truth应该是 “北，京，欢，迎，你，<e>”
1. 将\<s> 作为decoder的初始输入，将decoder的最大概率输出词向量A1和‘北’做cross entropy（交叉熵）计算error。
2. 将\<s>，“**北**” 作为decoder的输入，将decoder的最大概率输出词 A2 和‘京’做cross entropy计算error。
3. 将\<s>，**“北”，“京”** 作为decoder的输入，将decoder的最大概率输出词A3和’欢’ 做cross entropy计算error。
4. 将\<s>，**“北”，"京"，“欢”** 作为decoder的输入，将decoder最大概率输出词A4和‘迎’做cross entropy计算error。
5. 将\<s>，**“北”，"京"，“欢” ，“迎”** 作为decoder的输入，将decoder最大概率输出词A5和‘你’做cross entropy计算error。
6. 将\<s>，**“北”，"京"，“欢” ，“迎” ，“你”**作为decoder的输入，将decoder最大概率输出词A5和结束词<e>做cross entropy计算error。
那么并行的时候是怎么做的呢，我们会有一个mask矩阵在这叫sequence mask，因为他起到的作用是在decoder编码我们的target sequence的时候对每一个词的生成遮盖它之后的词的信息,形状上图所示。
这也就是为什么LLM可以用海量的数据进行自回归模型的训练，因为每个token的解码是可以并行的，这也是RNN无法做到的，唯一的挑战就是对于GPU的资源消耗。
同时，因为LLM训练和生成解码的自回归特性，如何使用有效的提示过程，就可以让LLM在解码的时候捕捉到更加丰富的“前情提要”信息，从而生成出更加准确且连贯的答案！

# 为什么需要提示过程？
以下是两个提示的示例：
## 1) 文章摘要

假设你正在阅读一篇关于佛罗里达州降雪的文章。你想快速了解文章的主要内容，因此你向 AI 展示你正在阅读的内容，并要求进行摘要：
佛罗里达州很少下雪，特别是在中部和南部地区。除了州的极北部地区外，佛罗里达州大部分主要城市都没有记录到可测量的降雪量，尽管记录到了少量的痕迹，或者每个世纪观测到几次空气中的飘雪。根据国家气象局的数据，在佛罗里达群岛和基韦斯特群岛自欧洲殖民以来没有发生过飘雪的情况，已有超过300年时间。在迈阿密、劳德代尔堡和棕榈滩，超过200年中只有一次关于在空气中观察到飘雪的报告，发生在1977年1月。在任何情况下，自这次1977年的事件以来，迈阿密、劳德代尔堡和棕榈滩都没有看到过飘雪的情况。

以下是 AI 的回复。更简洁易读！
佛罗里达州很少下雪，除了州的极北部地区外，在过去的200年中迈阿密、劳德代尔堡和棕榈滩这些主要城市中只有一次观察到空气中飘雪的报告。

## 2) 数学问题求解
如果你有一个数学方程，想让语言模型来解决，你可以通过提问 "数学方程等于几" 来输入提示。
对于一个给定的问题，你的完整提示可能是这样的：
965 * 590 等于几?

对于这个提示，GPT-3（一种 AI 模型）有时会回答 569,050（不正确）。
而如果我们不是问965 * 590 等于几？，而是问确保你的答案完全正确。965*590 等于几？确保你的答案完全正确：，GPT-3 将会回答 569,350（正确）这就是提示工程的重要性所在。
# 如何进行提示过程？
有很多种提示方式
## 角色提示：

一种提示技术是给 AI 分配一个角色。例如，你的提示可以以"你是一名医生"或"你是一名律师"开始，然后要求 AI 回答一些医学或法律问题。举个例子：
你是一个能解决世界上任何问题的杰出数学家。试着解决下面的问题：100*100/400*56 是多少？

AI (GPT-3 davinci-003) 的答案：
答案是 1400。

例如：
我想让你充当软件开发人员。我将提供一些关于 Web 应用程序要求的具体信息，您的工作是提出用于使用 Golang 和 Angular 开发安全应用程序的架构和代码。我的第一个要求是’我想要一个允许用户根据他们的角色注册和保存他们的车辆信息的系统，并且会有管理员，用户和公司角色。我希望系统使用 JWT 来确保安全。

更多角色提示语句见
- 中文：150种ChatGPT最佳实践提示模版 
- 英文：Awesome ChatGPT Prompts

## 多范例提示：
多范例提示（few shot prompting）, 这种策略将为模型展示一些例子（shots），从而更形象地描述你的需求。通过给出少量例子来对模型进行提示，从而使得模型快速得到想要的结果模式，从而在指定的范式下生成

不同类型的范例提示
单词 shot 在该场景下与 example（范例） 一致。除了多范例提示（few-shot prompting）之外，还有另外两种不同的类型。它们之间唯一的区别就是你向模型展示了多少范例。
类型:
- 无范例提示（0 shot prompting）: 不展示范例
- 单范例提示（1 shot prompting）: 只展示 1 条范例
- 多范例提示（few shot prompting）: 展示 2 条及以上的范例
### 无范例提示
无范例提示是最基本的提示形式。它仅仅是向模型展示提示信息，没有提供任何示例，并要求其生成回答。因此，你到目前为止看到的所有指令和角色提示都属于无范例提示。无范例提示的另一个例子是：
Add 2+2:

这是无范例提示，因为我们没有向模型展示任何完整的示例。
### 单范例提示
单范例提示是向模型展示一个示例。例如：
Add 3+3: 6
Add 2+2:

我们仅向模型展示了一个完整的示例（“Add 3+3: 6”），因此这是一个单范例提示。
### 多范例提示
多范例提示是向模型展示2个或更多示例。例如:
Add 3+3: 6
Add 5+5: 10
Add 2+2:

这是我们向模型展示了至少2个完整的示例（“Add 3+3: 6”和“Add 5+5: 10”）。通常，展示给模型的示例越多，输出结果就越好，因此在大多数情况下，多范例提示比另外两种提示更受欢迎。
## 组合提示：

以下是一个包含上下文、指令以及多示例提示的例子：
Twitter是一个社交媒体平台，用户可以发布称为“推文”的短消息。推文可以是积极的或消极的，我们希望能够将推文分类为积极或消极。以下是一些积极和消极推文的例子。请确保正确分类最后一个推文。
Q: 推文: "今天真是美好的一天！"这条推文是积极的还是消极的？
A: 积极的
Q: 推文: 我讨厌这个班级"这条推文是积极的还是消极的？
A: 消极的
Q: 推文: "我喜欢牛仔裤上的口袋"
A:

通过添加额外的上下文和示例，我们通常可以提高人工智能在不同任务上的表现。
## 规范化提示：
以下是在一个提示中将时常看到的一些组成部分：
- 角色
- 指令/任务
- 问题
- 上下文
- 示例(few shot)
我们发现，通过规范这样的语句往往可以提升LLM回答的准确程度
例如：
假如你是一名Python程序员。 
请编写python代码，生成3个单词数组（每个数组6个单词）分别包括介词、动词和名词，名词也用作直接宾语，从中选出指定的词组成一个句子，并确保可以执行。

输出结果可能像下面一样:

```python
import random
prepositions = ['at', 'in', 'on', 'with', 'under', 'over']
verbs = ['run', 'jump', 'dance', 'sing', 'laugh', 'cry']
nouns = ['cat', 'dog', 'tree', 'flower', 'book', 'computer']

preposition = random.choice(prepositions)
verb = random.choice(verbs)
noun = random.choice(nouns)

madlib = f"I {verb} {preposition} the {noun}."

print(madlib)
```


## 风格提示：
风格指导就是要求 AI 以某种特定的风格来表达。如果没有给出风格指导的问题，ChatGPT 通常会返回一两个短段落的回答，少数需要更长的回答时会返回更多内容。
一个更详细的风格提示的例子可能是：
[问题]“以拥有20多年经验和多个博士学位的[领域]专家的风格和水平写作。在回答中优先考虑有建设性的、不太知名的建议。使用详细的例子进行解释，尽量少离题和耍幽默。“
使用风格输入提示将大大提高回答的质量！
思维链提示：
思维链(CoT)提示过程是一种最近开发的提示方法，它鼓励大语言模型解释其推理过程。下图显示了 few shot standard prompt（左)与链式思维提示过程（右）的比较。

常规提示过程 vs 思维链提示过程(Wei et al.)
思维链的主要思想是通过向大语言模型展示一些少量的 exemplars，在样例中解释推理过程，大语言模型在回答提示时也会显示推理过程。这种推理的解释往往会引导出更准确的结果。由浅入深，提升问题质量
例子：
哪种方法是更快的回家方式？
选项1：乘坐10分钟的公共汽车，然后40分钟的公共汽车，最后10分钟的火车。
选项2：乘坐90分钟的火车，然后骑行45分钟，最后10分钟的公共汽车。

选项1需要60分钟，即10+40+10 

# 提示过程的应用
学会一些提示语句还需要在具体的实战中应用，提升工作效率
## 写邮件：
想象一下，有一天你生病了，不能去上班。这里有一个简单的提示，可以写一封电子邮件给你的老板，告诉他们你病了。
我的名字：克洛丝
老板的名字：凯尔希

写一封严肃、专业的电子邮件给我的老板，告诉他我今天因为生病不能上班。请简洁明了：

AI生成：
尊敬的凯尔希先生：

我是克洛丝，很抱歉地通知您，我今天因为生病不能上班。我会尽快恢复健康，并尽快回到工作岗位。

谢谢您的理解，

克洛丝

## 代码辅助：
作为程序员，生成代码，注释代码，debug代码都是效率很低的事情，那在ChatGPT的帮助下，我们可以做到更高效地完成这些操作
## 代码生成：
和上述例子相同：
假如你是一名Python程序员。 
请编写python代码，生成3个单词数组（每个数组6个单词）分别包括介词、动词和名词，名词也用作直接宾语，从中选出指定的词组成一个句子，并确保可以执行。

## 代码注释：
你可以要求ChatGPT为你的代码添加注释，并对其进行格式化以便阅读。在你的提示词指令之后，添加三个#号，然后粘贴你想要它清理的代码:
假如你是一名Python程序员。 请将下列Python代码添加行注释并重新构造代码结构以使其易于阅读：

## Debug代码：
ChatGPT不仅可以检测代码中的语法错误，还可以找到执行代码时会出现的逻辑错误。下面是一个Python脚本的例子，由于第3行的逻辑错误，在第4行最终会导致“除以零”错误。尝试使用以下简单的提示词来查找并修复错误:
假如你是一名资深的程序员，负责python程序的开发，请debug此Python代码，找出错误：

当然，提示学习有很多很多有趣的应用，我们可以使用一些公开的提示文档进行使用，从而提升工作效率


# 总结：
GPT不止是技术，会带来一场产业革命；AI替代不了人，但会用AI的人能替代你。
# 引用：

> *Transformer解读：https://blog.csdn.net/fs1341825137/article/details/120247499?spm=1001.2014.3001.5501
> 提示工程学习文档：https://www.promptingguide.ai/zh
> https://learnprompting.org/zh-Hans/docs/intro
> github开源项目：https://github.com/dair-ai/Prompt-Engineering-Guide
> 思维链提示论文：Wei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B., Xia,
> F., Chi, E., Le, Q., & Zhou, D. (2022). Chain of Thought Prompting
> Elicits Reasoning in Large Language Models.*
